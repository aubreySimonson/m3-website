<!DOCTYPE html>
<html>
    <head>
        <title>MAKING MUSIC IN THE METAVERSE</title>
        <link rel="stylesheet" href="stylesheets/styles.css">
    </head>
    <body>
      <!--------------------------------------------------------------Top Bit---------------------------------------------------------->
      <!-- reminder to put a cool graphic under this-->
      <h1>2.S972: Making Music in the Metaverse</h1>
      <h2>Fall, 2022</h2>
      <h2>Massachusetts Institute of Technology</h2>
      <p class = "introtext">

      </p>
      <!--------------------------------------------------------Menu (Sideways, Fancy)------------------------------------------------->
      <div class = "menu">
        <a class = "menuitem" href="#teachingteam">Teaching Team</a>
        <a class = "menuitem" href="#guestspeakers">Guest Speakers</a>
        <a class = "menuitem" href="#studentprojects">Student Projects</a>
      </div>
      <!-------------------------------------------------------------Teaching Team----------------------------------------------------->
      <div id = "teachingteam">
        <h2>Teaching Team</h2>
        <h3 class = "personname">Ken Zolot</h3>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
            Ken Zolot is a Senior Lecturer in the department of Mechanical Engineering at MIT,
            and a Professor of Creative Entrepreneurship at The Berklee College of Music.
            His other work can be found at http://www.mit.edu/people/zolot
          <span>
        </div>

        <h3 class = "personname">Aubrey Simonson</h3>
        <div class = "personcontainer">
          <img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">
          <span class = "persondescription">
            Aubrey is a Unity developer and VR interaction designer. He holds an MS from
            the MIT Media Lab, where he wrote the thesis
            <a href = "https://drive.google.com/file/d/1p6IUu9QIzWNBERz3IW_yVcojQjVz06rl/view">
              An Integrated System for Interaction in Virtual Environments
            </a>
            as a memeber of the Fluid Interfaces Group,
            and BAs from Wellesley College in Political Science and Media Arts and Sciences.
            His other work can be found at <a href = "https://aubreysimonson.gitlab.io/page/">aubreysimonson.com</a>.
            He also made this website :)
          <span>
        </div>

        <h3 class = "personname">Mateo </h3>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
            Put bio here!
          <span>
        </div>
      </div>
      <!------------------------------------------------------------Guest Speakers----------------------------------------------------->
      <div id = "guestspeakers">
        <h2>Guest Speakers</h2>

        <h3 class = "personname">Rus Gant</h3>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
            Description for Rus here
          </span>
          <span class = "persondescription">
            <h4>
              Reading assigned:
            </h4>
            <ul>
              <li>
                None :)
              </li>
            </ul>
          </span>
        </div>

        <h3 class = "personname">Lori Landay</h3>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
            Description for Lori here
          </span>
          <span class = "persondescription">
            <h4>
              Reading assigned:
            </h4>
            <ul>
              <li>
                <a href = "https://blog.leapmotion.com/art-storytelling-narrative-vr/">
                   The Art of Storytelling and Narrative in VR
                </a>
              </li>
            </ul>
          </span>
        </div>

        <h3 class = "personname">Iulian Radu</h3>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
            Description for Iulian here
          </span>
          <span class = "persondescription">
            <h4>
              Reading assigned:
            </h4>
            <ul>
              <li>
                <a href = "https://www.dropbox.com/s/4ammnst4zv3ts33/22.Chapter%2023%20Augmented%20reality-preprints.pdf?dl=0">
                   Augmented Reality in the Learning Sciences
                </a>
              </li>
            </ul>
          </span>
        </div>

        <h3 class = "personname">David Lobser</h3>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
            David Lobser is a 3D animator and VR artist.
            He taught 3D animation at Harvard University before shifting careers to XR development.
            He received a BFA from the School of Visual Arts (SVA) in NYC and
            a Master of Professional Studies (MPS) from the Interactive Telecommunications Program (ITP)
            at New York University's Tisch School of the Arts.
            Under Ken Perlin he worked as a researcher and artist-in-residence at NYU's "Future Reality Lab,"
            where he focused on multi-user, shared-space VR experiences.
            His recent work focuses on therapeutic uses for VR.
            He co-founded "Luxury Escapism," New York's first digital spa and developed a variety of VR,
            projection and physical computing therapies to fill it out, including "Cosmic Sugar" and "Visitations."
          </span>
          <span class = "persondescription">
            <h4>
              Reading assigned:
            </h4>
            <ul>
              <li>
                <a href = "https://cosmicsugarvr.com/">
                   VR Experience: Cosmic Sugar
                </a>
              </li>
            </ul>
          </span>
        </div>

        <h3 class = "personname">Sam Chin</h3>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
            Sam intro here!
          </span>
          <span class = "persondescription">
            <h4>
              Reading assigned:
            </h4>
            <ul>
              <li>
                <a href = "https://www.youtube.com/watch?v=4c1lqFXHvqI">
                   TED Talk: Can we create new senses for humans? | David Eagleman
                </a>
              </li>
            </ul>
          </span>
        </div>

        <h3 class = "personname">Erica Knowles</h3>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
            Erica intro here!
          </span>
          <span class = "persondescription">
            <h4>
              Reading assigned:
            </h4>
            <ul>
              <li>
                <a href = "https://mitpress.mit.edu/9780262582780/sweet-anticipation/">
                   Sweet Anticipation: Music and the Psychology of Expectation, Chapter 11: Genres, Schemas, and Firewalls
                </a>
              </li>
            </ul>
          </span>
        </div>

        <h3 class = "personname">Akito van Troyer</h3>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
            Akito intro here!
          </span>
          <span class = "persondescription">
            <h4>
              Reading assigned:
            </h4>
            <ul>
              <li>
                <a href = "https://mckittrickhotel.com/sleep-no-more/#/">
                   Sleep No More
                </a>
              </li>
              <li>
                <a href = "https://opera-beta.media.mit.edu/portfolio/sleep-no-more/">
                   Remote Theatrical Immersion
                </a>
              </li>
              <li>
                <a href = "https://opera-beta.media.mit.edu/portfolio/sleep-no-more/">
                   Remote Theatrical Immersion
                </a>
              </li>
              <li>
                <a href = "https://opera-beta.media.mit.edu/portfolio/city-symphonies/">
                   City Symphonies
                </a>
              </li>
              <li>
                <a href = "https://college.berklee.edu/news/electronic-production-design/electronic-production-and-design-students-create-dream-machine">
                   EPD Students Create Dream Machine, a Virtual Performance Experience
                </a>
              </li>
              <li>
                <a href = "https://college.berklee.edu/news/electronic-production-design/electronic-production-and-design-students-create-dream-machine">
                   EPD Students Create Dream Machine, a Virtual Performance Experience
                </a>
              </li>
              <li>
                <a href = "https://tangible.media.mit.edu/project/teleabsence/">
                   TeleAbsence
                </a>
              </li>
            </ul>
          </span>
        </div>

        <h3 class = "personname">Pattie Maes</h3>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
            Pattie intro here!
          </span>
          <span class = "persondescription">
            <h4>
              Reading assigned:
            </h4>
            <ul>
              <li>
                <a href = "https://en.wikipedia.org/wiki/Pattie_Maes">
                   Her Wikipedia page
                </a>
              </li>
              <li>
                <a href = "https://www.oscar-rosello.com/nevermind">
                   Nevermind
                </a>
              </li>
              <li>
                <a href = "https://www.ted.com/talks/pattie_maes_pranav_mistry_meet_the_sixthsense_interaction?language=en">
                   SixthSense
                </a>
              </li>
              <li>
                <a href = "https://www.media.mit.edu/projects/attentivu/overview/">
                   AttentiveU
                </a>
              </li>
            </ul>
          </span>
        </div>

        <h3 class = "personname">Andrzej Banburski-Fahey</h3>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
            Andy intro here!
          </span>
          <span class = "persondescription">
            <h4>
              Reading assigned:
            </h4>
            <ul>
              <li>
                <a href = "https://arxiv.org/abs/2211.05875">
                   Steps towards prompt-based creation of virtual worlds
                </a>
              </li>
            </ul>
          </span>
        </div>

        <h3 class = "personname">Andrzej Banburski-Fahey</h3>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
            Andy intro here!
          </span>
          <span class = "persondescription">
            <h4>
              Reading assigned:
            </h4>
            <ul>
              <li>
                <a href = "https://arxiv.org/abs/2211.05875">
                   Steps towards prompt-based creation of virtual worlds
                </a>
              </li>
            </ul>
          </span>
        </div>

      </div>
      <!----------------------------------------------------------Student Projects---------------------------------------------------->
      <div id = "studentprojects">
        <h2>Student Projects</h2>

        <h3 class = "teamname">MobiusWalk: Powered Omnidirectional Treadmill</h3>
        <!-- include video, log book, apk link -->
        <h4 class = "personname">Sabrina Hare</h4>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
             Hardware design reviews
          </span>
        </div>
        <h4 class = "personname">Louisa Wood</h4>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
             Unity development & hardware design
          </span>
        </div>
        <h4 class = "personname">Charles Young</h4>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
             Hardware & embedded system development
          </span>
        </div>

        <div class = "mediacontent">
          <p class = "projecttext">
            Omnidirectional treadmill with a unique structural design, a unified
            torus-like belt, that enables significantly lower cost and higher reliability.
          </p>
          <p class = "projecttext">
            In topology, a torus is homeomorphic to the cartesian product of two circles,
            which promises that one can go around the torus in two independent directions forever.
            Coincidentally, this is exactly what an omnidirectional treadmill needs.
            So we made our belt into this shape and designed drive modules accordingly.
          </p>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/Mobius/Mobius1.png' class = "imageinset">
            <img src = 'images/ofProjects/Mobius/Mobius2.png' class = "imageinset">
          </div>
          <p class = "projecttext">
            The advantage of this design is clear: significantly less moving parts
            that enables lower cost and higher reliability. But the process to make
            it work is not easy, see the logbook for details. To integrate our product
            into existing VR solutions, we implemented bluetooth connection in the onboard
            control chip to communicate with any headset that supports bluetooth.
          </p>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/Mobius/Mobius3.png' class = "imagealone">
          </div>
          <p class = "projecttext">
            For the control algorithm, we decided to implement a computer-vision-based solution,
            which provides the possibility to offer movement prediction and therefore faster response.
            For the current stage, we used opencv to identify visual fiducials, and we plan to replace
            it with a deep learning based pose estimation solution.
          </p>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/Mobius/Mobius4.png' class = "imagealone">
          </div>
          <div class = "imagescontainer">
            <iframe width="1120vw" height="630vw" src="https://www.youtube.com/embed/WINyYgfF3Fo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
          <!-- <p class = "projecttext">
            <span class = "emphasis">APK link: </span> REMINDER TO ADD LINK
          </P> -->
          <p class = "projecttext">
            <span class = "emphasis">Log book:</span>
            <a href = "https://docs.google.com/document/d/1nOo4zDN24DHwQ3Ybsj5JdggHba-XH2AUskDlep-x21o/edit">
              https://docs.google.com/document/d/1nOo4zDN24DHwQ3Ybsj5JdggHba-XH2AUskDlep-x21o/edit
            </a>
          </P>
        </div>


        <h3 class = "teamname">Walk This Way</h3>
        <!--- Add full write up link, apk link, demo video  -->
        <h4 class = "personname">Daniel Halis</h4>
        <h4 class = "personname">Xinyi “Cindy” Yang</h4>

        <div class = "mediacontent">
          <p class = "projecttext">
            We propose “Haptic Nudges,” an extension to redirected walking (Razzaque et al. 2002) techniques
            to provide an intuitive, unobtrusive understanding of the real-world space. Haptic Nudges maximise
            virtual traversal distance while increasing agency. In addition, we update existing Open Source redi-
            rected walking toolkits (Azmandian et al. 2016), (Li et al. 2021) with new redirection algorithms
            and expose artificial potential fields (Dong et al. 2020) for all techniques. The combination of these
            techniques can be applied as a drop-in addition to any existing Unity project.
          </p>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/Walk/Walk1.png' class = "imageinset">
            <img src = 'images/ofProjects/Walk/Walk2.png' class = "imageinset">
          </div>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/Walk/Walk3.png' class = "imageinset">
            <img src = 'images/ofProjects/Walk/Walk4.png' class = "imageinset">
          </div>
          <!-- <p class = "projecttext">
            <span class = "emphasis">APK link: </span> REMINDER TO ADD LINK
          </P> -->
          <p class = "projecttext">
            <span class = "emphasis">Full project write up: </span>
            <a href = "files/writeup.pdf">click to download</a>
          </P>
        </div>

        <!-- Some team-related media here -->

        <h3 class = "teamname">Museum of the Mind</h3>
        <h4 class = "personname">Alice Cai</h4>
        <h4 class = "personname">Gloria Kang</h4>
        <h4 class = "personname">Lucy Nester</h4>

        <div class = "mediacontent">
          <p class = "projecttext">
            Museum of the Mind is an interactive, AI-assisted VR ideation space.
            It allows users to explore a museum and co-create art galleries with
            an AI in an adaptive environment. Users input ideas they want to explore
            with text-to-speech which is then run through GPT3 to generate related
            text and a diffusion model to generate related images in real time, in
            front of the user. EEG data is simultaneously used to manipulate features
            of the environment to increase immersion. Because of the generative and
            personalized nature of Museum of the Mind, every session is completely unique.
          </p>
          <p class = "projecttext">
            Museum of the Mind in an ideation space which is intended to let users explore
            and visualize new ideas. It takes advantage of VR as a way to create and explore
            spaces which don’t exist in reality, and the environment is intentionally epic
            and surreal to help users get into a new mindspace. After beginning a new session,
            users ideate using voice input–anything from keywords to whole sentences. This input
            is then run through GPT3 to expand on the idea and generate image captions.
            By utilizing GPT3’s natural language generation to create these image captions,
            we let the human and the AI co-create, showing users something new to build off of or
            play with as they ideate about their concept. The image captions are then passed to our server,
            which uses a diffusion model to generate images, each of which will be completely unique.
            Users can then continue on in the space giving a new prompt or expanding on previous exhibits
            in each gallery.
          </p>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/Museum/Museum1.png' class = "imageinset">
            <img src = 'images/ofProjects/Museum/Museum2.png' class = "imageinset">
          </div>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/Museum/Museum3.png' class = "imageinset">
            <img src = 'images/ofProjects/Museum/Museum4.png' class = "imageinset">
          </div>
          <!-- <p class = "projecttext">
            <span class = "emphasis">APK link: </span> REMINDER TO ADD LINK
          </P> -->
        </div>



        <h3 class = "teamname">SlideMill</h3>
        <!-- Add apk links, demo video from final assignment -->
        <h4 class = "personname">Jack Eastman</h4>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
             Embedded devices and Integration
          </span>
        </div>
        <h4 class = "personname">Jack Lewis</h4>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
             Unity Developer
          </span>
        </div>
        <h4 class = "personname">AhnPhu Nguyen</h4>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
             Treadmill Engineer
          </span>
        </div>
        <h4 class = "personname">Luka Srsic</h4>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
             Treadmill Engineer
          </span>
        </div>

        <div class = "mediacontent">
          <p class = "projecttext">
            Slidemill is an immersive device aimed at bringing cheaper and more accessible virtual
            reality treadmills to the market. Utilizing a DIY treadmill design, along with accessible
            electronics, we were able to create an omnidirectional passive treadmill that interfaces with
            the Oculus Quest 2 to allow users to control locomotion in virtual reality using their real legs,
            walking. With such a design, virtual reality becomes infinitely more immersive, with your
            motions in the real world being tracked and translated into the virtual space. By syncing real
            world and virtual movements this also goes a long way to counter one of virtual reality’s
            biggest problems: motion sickness.
          </p>
          <p class = "projecttext">
            Our project centers around the idea of an omnidirectional treadmill, a treadmill upon
            which a user can walk indefinitely in any direction. This would allow the user to have much
            more immersion, being able to walk themselves around their virtual space without limits,
            rather than needing to utilize a joystick or teleportation locomotion. To accomplish this, we
            built a low cost treadmill system that employs slippery materials in order to allow a user to
            slide their feet along the surface. The user is held in place with a support system, preventing
            them from moving off of the treadmill or falling down. Two slippery shoe covers are provided
            for the user’s feet, to enhance the sliding, as well as ensure that the experience would be the
            same across users, while still allowing versatility in shoe choice without limiting accessibility.
            The treadmill was constructed primarily with spare wood, utilizing a large machine bearing to
            allow the user’s support system to turn with them. Many of the friction reducing surfaces were
            made from pieces of whiteboard, a material students are likely to be able to find for free. This
            low cost, DIY approach to building the treadmill will hopefully inspire more makers and creators
            to look more into the underserved market.
          </p>
          <p class = "projecttext">
            To integrate the motions of the user into the virtual reality space, we utilized two ESP32
            microcontrollers, communicating with an icm20948 inertial measurement unit, one for each
            foot. In addition, there is a button placed on the bottom of each foot to ensure that the user’s
            movement data was more accurate and that their motions would only be captured if their feet
            were on the ground. The devices would be attached to the user's foot to measure acceleration,
            rotation, and magnetometer data. The data from this was then filtered in order to provide
            smooth measurements to the Quest 2. On the Quest, these measurements were interpreted
            into locomotion data for the user. Then, the Quest, using this data, would move the user
            around in the virtual world, according to the measurements from the ESP32.
          </p>
          <p class = "projecttext">
            Combining the omnidirectional passive treadmill and the foot tracking devices, one
            could theoretically walk for an infinite amount in any direction, thus greatly increasing the
            immersion of their virtual reality experience. Without the limitations of a user’s physical space,
            immersion is greatly increased. Users are no longer bound by the confines of their rooms, and
            instead can explore the virtual world, walking and running without constraints. In our vision,
            we wanted to bring this experience to more users, to make it more accessible. In this project,
            we believe that we have demonstrated that this is possible. Next generation VR locomotion can
            be made accessible to more users.
          </p>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/SlideMill/SlideMill1.jpg' class = "imageinset">
            <img src = 'images/ofProjects/SlideMill/SlideMill2.png' class = "imageinset">
          </div>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/SlideMill/SlideMill4.png' class = "imageinset">
          </div>
          <!-- <p class = "projecttext">
            <span class = "emphasis">APK link: </span> REMINDER TO ADD LINK
          </P> -->
        </div>


        <h3 class = "teamname">AR Storybook</h3>
        <!---Add apk link-->
        <h4 class = "personname">Margaret Wang</h4>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
        </div>
        <h4 class = "personname">Preston Bezos</h4>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
        </div>

        <div class = "mediacontent">
          <p class = "projecttext">
            In this project we have created an AR application that merges computer vision and
            storytelling to augment beloved storybooks. Users are able to point their phones at an
            existing children’s book (for this project, “The Snowy Day” by Ezra Jack Keats) and the
            characters and objects in the book come to life as 3D assets before their eyes. This 3D
            representation emerges perpendicular from the book, and can be viewed from all angles
            by moving the book or phone around. The user can admire the art and read the story,
            and when they are ready, flip the page for the next element of the story.
          </p>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/ARStorybook/ARStorybook1.png' class = "imageinset">
            <img src = 'images/ofProjects/ARStorybook/ARStorybook2.png' class = "imageinset">
          </div>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/ARStorybook/ARStorybook3.png' class = "imageinset">
            <img src = 'images/ofProjects/ARStorybook/ARStorybook4.png' class = "imageinset">
          </div>
          <!-- <p class = "projecttext">
            <span class = "emphasis">APK link: </span> REMINDER TO ADD LINK
          </P> -->
        </div>



        <h3 class = "teamname">Rhythm Rider</h3>
        <h4 class = "personname">Andrew Emmel</h4>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
             Flex/Visuals
          </span>
        </div>
        <h4 class = "personname">Haley Higginbotham</h4>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
             Hardware
          </span>
        </div>
        <h4 class = "personname">Livia Zhang</h4>
        <div class = "personcontainer">
          <!---<img src = 'images/ofPeople/AubreySimonson.jpg' class = "portrait">-->
          <span class = "persondescription">
             Software
          </span>
        </div>
        <div class = "mediacontent">
          <p class = "projecttext">
            Ride away to your favorite songs on an actual skateboard, in VR! We have
            developed a controller built on top of a skateboard that, when paired along with the
            experience we developed, allows you to move your character in-game. Simply tilt
            yourself on the skateboard in order to move your character from side to side in-game,
            while also trying to ride over the notes.
          </p>
          <p class = "projecttext">
            We designed a controller with a skateboard deck as a base. Basically, it has an
            IMU to sense changes in rotation on the skateboard. This IMU communicates with an
            Arduino, which then communicates with the Quest. Changes in rotation are then
            translated into horizontal movement for the player in-game.
          </p>
          <p class = "projecttext">
            As for the game itself, it takes place in a zen-like environment, with buildings that
            pop out of the ocean upon hitting notes. These notes are automatically placed as you
            play a song, meaning you can put any song onto your VR headset and play it in game.
            We achieve this by analyzing the song and finding where the notes land, and then
            placing notes on the musical staff/track.
          </p>
          <p class = "projecttext">
            We’re hoping the mix of a unique physical interaction alongside fun gameplay will
            create a compelling demo for everybody!
          </p>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/Rhythm/Rhythm1.png' class = "imageinset">
            <img src = 'images/ofProjects/Rhythm/Rhythm2.png' class = "imageinset">
          </div>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/Rhythm/Rhythm3.png' class = "imagealone">
          </div>
          <!-- <p class = "projecttext">
            <span class = "emphasis">APK link: </span> REMINDER TO ADD LINK
          </P> -->
        </div>


        <h3 class = "teamname">ESCAPE: the Quest</h3>
        <!-- Add apk links, demo video from final assignment -->
        <h4 class = "personname">Emeka Ezike</h4>
        <h4 class = "personname">Jared Scott</h4>
        <h4 class = "personname">Henry Smith</h4>
        <h4 class = "personname">Eli Villa</h4>

        <div class = "mediacontent">
          <p class = "projecttext">
            ESCAPE: The Quest is a virtual reality escape room that uses various
            bio signals to make for a more immersive and interactive experience.
            The escape room consists of a set of challenges, each with a different
            biosignal hardware component. Particularly, one challenge uses a
            Brain Computer Interface that was implemented using a Mindflex headband.
            One challenge uses a heartbeat monitor to provide audio and visual feedback to the user.
            Although puzzle games and escape rooms have been done in VR, the use of biosignals
            makes for a novel experience that could be replicated in future VR projects.
          </p>
          <p class = "projecttext">
            You start in a broken down spaceship that has been taken over by an evil AI.
            Your goal is to repair the ship and find a way to defeat the AI before it’s too late!
            You wake up to alarms blaring and no power to the ship. You must first restore power
            by turning the engine back on. This is done by manually overriding the engine with
            your brain power (BCI puzzle). Then you must find the secret passcode that will cure the AI.
            \It is on a satellite near the ship that you must navigate to using your thrusters (Debris Challenge).
            However, there is debris flying around that you must avoid! Hearing and feeling your heartbeat puts
            the pressure on you to not make a mistake.
          </p>
          <p class = "projecttext">
            Once the passcode is safe in hand, you go to the secret room and put on the virtual reality
            goggles to enter the metaverse and input the passcode (AR puzzle).
            Then you have cured the AI and can return home.
          </p>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/ESCAPE/ESCAPE1.png' class = "imageinset">
            <img src = 'images/ofProjects/ESCAPE/ESCAPE2.png' class = "imageinset">
          </div>
          <div class = "imagescontainer">
            <img src = 'images/ofProjects/ESCAPE/ESCAPE3.png' class = "imageinset">
            <img src = 'images/ofProjects/ESCAPE/ESCAPE4.png' class = "imageinset">
          </div>
          <!-- <p class = "projecttext">
            <span class = "emphasis">APK link: </span> REMINDER TO ADD LINK
          </P> -->
        </div>
      </div>
    </body>
  </html>
